---
title: "Day Three: Data Analysis"
author: "Dillon Niederhut"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
- pdf_document
- slidy_presentation
---

```{r, echo=FALSE}
knitr::opts_knit$set(root.dir = '../')
```

## Pre-introduction

While everyone is getting situated and/or cloning the course materials, pull up `feedback_cleaner.R`. As a review of day 3, walk through the different parts of the script, and ask the students to describe to you what each piece does. For example, 

```
dat$timestamp <- sub(' [0-9]+:[0-9]+:[0-9]+', '', dat$timestamp)
dat$timestamp <- as.Date(dat$timestamp, "%m/%d/%Y")
```

is code that reformats ISO timestamps so that R can read them as date-type values.

## Introduction

analysis generally procedes in two steps:

1. exploratory data analysis
2. statistical inference

our treatment of graphing owes a lot to the Grammar of Graphics 

# Summarizing

## let's load in some data about D-Lab feedback 

```{r}
load('data/feedback.Rda')
str(dat)
```

## R provides two easy/simple summary functions in the base package

```{r}
summary(dat)
table(dat$department)
```

think back to day one - how would we make weekdays out of the date variable?

```{r}
dat$wday <- factor(weekdays(dat$timestamp, abbreviate = TRUE), 
                   levels = c('Mon','Tue','Wed','Thu','Fri','Sat','Sun')
                   )
summary(dat$wday)
```

> side note - `weekdays` is locale aware, so students who have their laptop language set to something other than english will get their weekday names in the other language

## reshape provides a few more ways to aggregate things

```{r}
library(reshape2)
dcast(dat[dat$gender == 'Female/Woman' | dat$gender == 'Male/Man',], department ~ gender)
dcast(melt(dat, measure.vars = c('course.delivered')), wday ~ 'Delivered', fun.aggregate = mean)
```

# Plotting

## every time you use `base::plot`, [Edward Tufte does something unkind to a cute animal](http://markandrewgoetz.com/blog/2009/11/my-new-wallpaper/)

- we'll be using ggplot, R's implementation of the **grammar of graphics**

- in this grammar, you use 'aesthetics' to define how data is mapped to objects the graph space

- each graph space has at least three layers:
    - theme/background/annotations
    - axes
    - objects

- most objects are geometric shapes

- some objects are statistics built on those shapes

- you can stack as many layers as you like

```{r}
install.packages('ggplot2')
library(ggplot2)
```

## use qplot for initial poking around

it has very strong intuitions about what you want to see, and is not particularly customizable

```{r}
qplot(instructor.communicated, data = dat)
qplot(wday, course.delivered, data = dat)
```

## for 1D categorical, use bar

```{r}
ggplot(data=dat, aes(x=wday)) + geom_bar()
```

## for 1D continuous, use hist

this is really just convenience for `geom_bar(stat = 'bin')`, as opposed to bar plots, whose `stat` is `'count'`

```{r}
ggplot(data=dat, aes(x=course.delivered)) + 
  geom_histogram(binwidth=1)
```

you can add color to this plot

```{r}
ggplot(data=dat, aes(x=course.delivered)) + 
  geom_histogram(binwidth=1, fill = 'gold', colour= 'blue')
```

GO BEARS

## for many 1D variables, use a box plot

these are handy for a whole bunch of reasons, and you should make them your close associates

```{r}
ggplot(data=dat, aes(x=gender,y=interest)) + geom_boxplot()
```

## to plot two continuous variables, use points

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + geom_point()
```

all of these values are discrete, which makes them hard to see

## to scatter points randomy, use jitter

this is really just convenience for `geom_point(position = jitter())` 

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter()
```

not only can you add color, you can make the color a mapping of other variables

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter(aes(colour = wday))
```

the last time we used `colour` it was not an aesthetic - why is it now?

## you can stack layers until your eyes hurt

```{r}
ggplot(data=dat, aes(x=wday, y=course.delivered)) + 
  geom_boxplot(colour = 'gold') + 
  geom_jitter(colour = 'blue')
```

## add summary functions with smooth

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm')
```

if you are using colour as an aesthetic, you'll produce stats for each color

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered, colour = wday)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm', se = FALSE)
```

## good scientists put units on their axes

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm', colour = 'black') + 
  xlab('How well the instructor communicated (1-7)') + 
  ylab('How well the course delivered advertised content (1-7)') + 
  ggtitle("I have no idea what I'm doing") 
```

the general point here is that every single object on this graph is customizable

frequent customizations are very simple to add

infrequent customizations will take a lot of tinkering on your part

## facetting

often useful for looking at relationships between three variables at the same time

```{r}
ggplot(data=dat, aes(x=instructor.communicated, y=course.delivered)) + 
  geom_jitter() + 
  stat_smooth(method = 'lm') +
  facet_grid(. ~ useful)
```

# Mean testing

a picture is worth 1,000 words, but a p-value is worth a dissertation

basically, inferential statistics is the application of probability theory to decide what is real and what isn't

we'll start by trying to tell whether differences between group summaries are real

## t.test with two vectors (default method)

```{r}
t.test(dat$inside.barriers, dat$outside.barriers)
```

note that R takes care of the defaults for you - what it is really computing is `t.test(dat$inside.barriers, dat$outside.barriers, alternative = "two.sided", paired = FALSE, var.equal = FALSE, mu = 0, conf.level = 0.95)

how would you find this out for yourself?

## t.test with subsets of one vector (default method)

```{r}
t.test(dat$outside.barriers[dat$gender == "Male/Man"], dat$outside.barriers[dat$gender == "Female/Woman"])
```

recall that we mentioned inconsistency on day one - here it is, and in a big way

## t.test with S3 method

```{r}
t.test(outside.barriers ~ gender, data = dat, subset = dat$gender %in% c("Male/Man", "Female/Woman"))
```

## aov

first, you would think anova would be called by `anova`, but that's reserved for conducting F-tests on lm objects

second, you really shouldn't be using anova, but if you must do it in R, the syntax looks like this

> side note - ANOVA was invented by Ron Fisher to make it easy to do linear models with only a pencil and paper, and has been superceded by regression since the advent of computation in the 70s

```{r}
aov(outside.barriers ~ gender, data = dat)
```

this isn't particularly helpful, but remember that it is an object, and we can call other, more helpful functions, on that object

remember our old friend `summary`? it works on almost everything

```{r}
model.1 <- aov(outside.barriers ~ gender, data = dat)
summary(model.1)
```

that's a little better - but what about post-hoc testing?

```{r}
TukeyHSD(model.1)
```

> side note - apparently Stata stores all of the models that you generate, whether you assign them names or not; in R, you must explicitly give your models names or they will disappear into the ether

# linear models

mean tests are really just a subset of linear models where your predictor is a category

## cor.test (Pearson)
 
earlier, we were looking at differences between the means of two variables

but those variables were both continuous, so we can ask whether they are related

```{r}
cor.test(dat$outside.barriers, dat$inside.barriers)
```

okay, so they're related - now what?

## lm

this is probably the closest you will get to building a linear model by hand

this means lm is a powerful tool, but you have to know what you're doing

the basic call is the S3 method

```{r}
model.1 <- lm(inside.barriers ~ outside.barriers, data = dat)
summary(model.1)
```

## R automatically one-hot encodes your categories

```{r}
model.2 <- lm(inside.barriers ~ outside.barriers + department, data = dat)
summary(model.2)
```

## R does not assume you want the full factorial model

```{r}
model.3 <- lm(inside.barriers ~ outside.barriers + department + outside.barriers*department, data = dat)
summary(model.3)
```

## extract model parameters with `$`

```{r}
model.1$coefficients
model.1$coefficients[[2]]
```

## this is useful if you want to plot residuals

```{r, eval=FALSE}
dat$residuals <- model.1$residuals
```

oh boy golly gee gosh darn! remember how we talked about R having casewise deletion + bad indexing? this is one place where it makes your life difficult

we have to do something like this:

```{r}
dat.listwise <- dat[!is.na(dat$inside.barriers) & !is.na(dat$outside.barriers), ]
dat.listwise$resid <- model.1$residuals
```

then we can do this

```{r}
ggplot(data = dat.listwise, aes(x=gender,y=resid)) + 
  geom_boxplot()
```

# Nonparametric

parametric refers to using means, deviations, and other estimates of population parameters

*BUT* what if you don't want to make assumptions about the structure of the population?

or what if you **gasp** can't?

## ranked variables

a simple case is where means don't have meaning

above we were looking at correlations between Likert variables

all Likerts are really rank variables, which means they don't act like actual number-y numbers

in the real world, a 6 foot tall person is twice as tall as a 3 foot tall person

but is a level '6' really twice as many barriers to access as a '3'?

**NOPE**

we know that 6 is more than 3, but can't really say how much - in that sense then, a scale of 1-7 is exactly the same thing as a scale of a-g.

## median testing ranks

we use Mann-Whitney sums to test that the ranks are centered the same way

```{r}
wilcox.test(dat$outside.barriers, dat$inside.barriers, alternative = "two.sided", paired = FALSE, mu = 0, conf.level = 0.95)
```

see how this setup looks exactly like a t-test? that's not an accident

## correlating ranks

this is just like the `cor.test` you did above, but with `method` set to equal 'spearman' instead of pearson

```{r}
cor.test(dat$outside.barriers, dat$inside.barriers, method = 'spearman')
```

rho is pretty close to the r from above

## chisq

what if both of your variables are categories? we can test their counts with R's built in `chisq.test` function

i.e. what if we want to know if gender is distributed evenly over departments?

```{r}
chisq.test(dat$gender, dat$department)
```

# Practice

## Assignment

There were a lot of variables in this dataset that we did not look at today:

```{r}
names(data)
```

Choose two of those variables, and explore their distribution and relationship to each other. Can you conclude anything about the D-Lab based on the feedback?

# Acknowledgements

## Materials taken from:

[D-Lab's Feedback Analytics](https://github.com/dlab-berkeley/feedback-analytics)